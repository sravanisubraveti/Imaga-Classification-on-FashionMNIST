{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction with Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import*\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn\n",
    "import time\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Dataset from Kaggle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from google.colab import files\n",
    "import io\n",
    "uploaded = files.upload()\n",
    "!pip install -q kaggle\n",
    "\n",
    "# Let's make sure the kaggle.json file is present.\n",
    "!ls -lha kaggle.json\n",
    "\n",
    "# Next, install the Kaggle API client.\n",
    "!pip install -q kaggle\n",
    "\n",
    "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
    "# so move it there.\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# This permissions change avoids a warning on Kaggle tool startup.\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "!kaggle competitions download -c ece-657a-w20-asg3-part-1\n",
    "\n",
    "!unzip train.csv.zip\n",
    "!unzip testX.csv.zip\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('testX.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing and Splitting Dataset and Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= train_data.iloc[:, 2:786].values\n",
    "Y= train_data.iloc[:,1].values\n",
    "print(X.shape)\n",
    "\n",
    "X_test= test_data.iloc[:,1:].values\n",
    "print(X_test.shape)\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "X_std_test = StandardScaler().fit_transform(X_test)\n",
    "print(X_std.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding explained variance for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explained variance for PCA\n",
    "pca = PCA(n_components=200, random_state= 42)\n",
    "X_std = pca.fit_transform(X)\n",
    "X_test=pca.transform(X_std_test)\n",
    "\n",
    "pca.n_components=200\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cum_var_explained=np.cumsum(explained_variance)\n",
    "print(cum_var_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cum_var_explained)\n",
    "plt.axis('tight')  \n",
    "plt.grid() \n",
    "plt.xlabel('n_components') \n",
    "plt.ylabel('Cumulative_Variance_explained')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Hyperparameters for Sample dataset for PCA+ SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_rem1, Y_train1, Y_rem1 = train_test_split(X, Y, test_size=0.83333, random_state=42)\n",
    "# Standardising the data\n",
    "X_train1_std = StandardScaler().fit_transform(X_train1)\n",
    "X_std_test = StandardScaler().fit_transform(X_std_test)\n",
    "X_train1_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1,0.5,1,2,5,10,20,50], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf']\n",
    "              \n",
    "#Implementing PCA with SVM for 10000 samples for 200 features\n",
    "start_time = time.clock()\n",
    "pca = PCA(n_components=200, svd_solver='randomized',\n",
    "whiten=True,random_state=42)\n",
    "X_train1_pca = pca.fit_transform(X_train1_std)\n",
    "X_test_pca = pca.fit_transform(X_std_test)\n",
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n",
    "grid.fit(X_train1_pca,Y_train1)\n",
    "print(grid.best_estimator_)\n",
    "grid_predictions = grid.predict(X_test_pca)\n",
    "print(grid_predictions)\n",
    "\n",
    "time= time.clock() - start_time\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Hyperparameters for Sample dataset for PCA+RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_rem1, Y_train1, Y_rem1 = train_test_split(X, Y, test_size=0.83333, random_stat\n",
    "(10000, 784)\n",
    "# Standardising the data\n",
    "X_train1_std = StandardScaler().fit_transform(X_train1)\n",
    "X_std_test = StandardScaler().fit_transform(X_std_test)\n",
    "X_train1_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "pca = PCA(n_components=200, svd_solver='randomized',whiten=True,random_state=42)\n",
    "X_train1_pca = pca.fit_transform(X_train1_std)\n",
    "X_test_pca = pca.fit_transform(X_std_test)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 1000, num = 100)]\n",
    "max_features = ['log2', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "param_dist = {'n_estimators': n_estimators,'max_features': max_features,'max_depth': max_depth,'bootstrap': bootstrap}\n",
    "print(param_dist)\n",
    "rs = RandomizedSearchCV(estimator= rf,param_distributions = param_dist,n_iter = 100,\n",
    "                        cv = 3,verbose = 1,n_jobs=-1,random_state=42)\n",
    "rs.fit(X_train1_pca, Y_train1)\n",
    "print(rs.best_params_)\n",
    "\n",
    "rs_predictions = rs.predict(X_test_pca)\n",
    "print(rs_predictions)\n",
    "\n",
    "time= time.clock() - start_time\n",
    "print(\" Time in Seconds: \",time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Explained Variance for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = StandardScaler().fit_transform(X)\n",
    "X_std_test = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=4)\n",
    "X_lda = lda.fit_transform(X_std,Y)\n",
    "X_std_test_full = lda.transform(X_std_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.n_components=4\n",
    "explained_variance = lda.explained_variance_ratio_\n",
    "cum_var_explained=np.cumsum(explained_variance)\n",
    "print(cum_var_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cum_var_explained)\n",
    "plt.axis('tight')\n",
    "plt.grid()\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Cumulative_Variance_explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Hyperparameters for Sample dataset for LDA with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_rem1, Y_train1, Y_rem1 = train_test_split(X, Y, test_size=0.83333, random_stat\n",
    "(10000, 784)\n",
    "# Standardising the data\n",
    "X_train1_std = StandardScaler().fit_transform(X_train1)\n",
    "X_std_test = StandardScaler().fit_transform(X_test)\n",
    "X_train1_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1,0.5,1,2,5,10,20,50], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf']\n",
    "              \n",
    "#Implementing PCA with SVM for 10000 samples for 200 features\n",
    "lda = LinearDiscriminantAnalysis(n_components=4)\n",
    "X_train1_lda = lda.fit_transform(X_train1_std,Y_train1)\n",
    "X_test_lda = lda.transform(X_std_test)\n",
    "              \n",
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n",
    "grid.fit(X_train1_lda,Y_train1)\n",
    "print(grid.best_estimator_)\n",
    "grid_predictions = grid.predict(X_test_lda)\n",
    "\n",
    "print(grid_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PCA with SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default SVM with PCA=200 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= train_data.iloc[:, 2:786].values\n",
    "Y= train_data.iloc[:,1].values\n",
    "print(X.shape)\n",
    "\n",
    "X_test= test_data.iloc[:,1:].values\n",
    "print(X_test.shape)\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "X_std_test = StandardScaler().fit_transform(X_test)\n",
    "print(X_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "pca = PCA(n_components=200, svd_solver='randomized',\n",
    "whiten=True,random_state=42)\n",
    "pca.fit(X_std)\n",
    "X_train_pca = pca.transform(X_std)\n",
    "X_test_pca = pca.transform(X_std_test)\n",
    "clf = SVC()\n",
    "\n",
    "scores=cross_val_score(clf, X_train_pca,Y, cv=10)\n",
    "print(scores)\n",
    "clf.fit(X_train_pca,Y)\n",
    "clf_predictions = clf.predict(X_test_pca)\n",
    "print(clf_predictions)\n",
    "\n",
    "time= time.clock() - start_time\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA=100, SVM(C=60,kernel=rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "pca = PCA(n_components=100, svd_solver='randomized',\n",
    "whiten=True,random_state=42)\n",
    "pca.fit(X_std)\n",
    "X_train_pca = pca.transform(X_std)\n",
    "X_test_pca = pca.transform(X_std_test)\n",
    "clf = SVC(C=60,kernel=rbf,random_state=42)\n",
    "\n",
    "scores=cross_val_score(clf, X_train_pca,Y, cv=10)\n",
    "print(scores)\n",
    "clf.fit(X_train_pca,Y)\n",
    "clf_predictions = clf.predict(X_test_pca)\n",
    "print(clf_predictions)\n",
    "\n",
    "time= time.clock() - start_time\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA=50, SVM(C=60,kernel=rbf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing PCA with SVM for 60000 samples for 50 features\n",
    "start_time = time.clock()\n",
    "\n",
    "pca = PCA(n_components=50, svd_solver='randomized',whiten=True,random_state=42)\n",
    "pca.fit(X_std)\n",
    "X_train_pca = pca.transform(X_std)\n",
    "X_test_pca = pca.transform(X_std_test)\n",
    "\n",
    "clf = SVC(C = 60, kernel = 'rbf')\n",
    "scores=cross_val_score(clf, X_train_pca,Y, cv=10)\n",
    "print(\"Mean accuracy score when C = {0} is {1}\".format(60,scores.mean()))\n",
    "clf.fit(X_train_pca, Y)\n",
    "\n",
    "predictions = clf.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA=50, SVM(C = 20, kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing PCA with SVM for 60000 samples for 50 features \n",
    "start_time = time.clock()\n",
    "pca = PCA(n_components=50,random_state=42)\n",
    "\n",
    "pca.fit(X_std)\n",
    "X_train_pca = pca.transform(X_std)\n",
    "X_test_pca = pca.transform(X_std_test)\n",
    "clf = SVC(C = 20, kernel = 'rbf',random_state=42)\n",
    "scores=cross_val_score(clf, X_train_pca,Y, cv=10)\n",
    "print(\"Mean accuracy score when C = {0} is {1}\".format(20,scores.mean()))\n",
    "\n",
    "clf.fit(X_train_pca, Y)\n",
    "\n",
    "\n",
    "predictions = clf.predict(X_test_pca)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "end_time = time.clock() - start_time\n",
    "print(\"time in seconds: \",end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PCA +Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= train_data.iloc[:, 2:786].values\n",
    "Y= train_data.iloc[:,1].values\n",
    "print(X.shape)\n",
    "\n",
    "X_test= test_data.iloc[:,1:].values\n",
    "print(X_test.shape)\n",
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "X_std_test = StandardScaler().fit_transform(X_test)\n",
    "print(X_std.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA=200, Random forest(n_estimators= 800, max_depth=70,max_features='sqrt',random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "pca = PCA(n_components=200,random_state=42)\n",
    "\n",
    "pca.fit(X_std)\n",
    "X_train_pca = pca.transform(X_std)\n",
    "X_test_pca = pca.transform(X_std_test)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators= 800, max_depth=70,max_features='sqrt',random_state=42)\n",
    "rf.fit(X_train_pca, Y)\n",
    "predictions = rf.predict(X_test_pca)\n",
    "print(predictions)\n",
    "\n",
    "time= time.clock() - start_time\n",
    "print(\" Time in mins: \",time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA=200, Random forest(n_estimators= 100, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "pca = PCA(n_components=50,random_state=42)\n",
    "\n",
    "pca.fit(X_std)\n",
    "X_train_pca = pca.transform(X_std)\n",
    "X_test_pca = pca.transform(X_std_test)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators= 100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train_pca, Y)\n",
    "predictions = rf.predict(X_test_pca)\n",
    "print(predictions)\n",
    "\n",
    "time= time.clock() - start_time\n",
    "print(\" Time in mins: \",time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PCA +Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA=200, Default GBC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "pca = PCA(n_components=200, svd_solver='randomized',\n",
    "whiten=True,random_state=42)\n",
    "pca.fit(X_std)\n",
    "X_train_pca = pca.transform(X_std)\n",
    "X_test_pca = pca.transform(X_std_test)\n",
    "gbgs = GradientBoostingClassifier()\n",
    "scores=cross_val_score(gbgs, X_train_pca,Y, cv=10)\n",
    "print(scores)\n",
    "gbgs.fit(X_train_pca,Y)\n",
    "#print(gbgs.best_params_)\n",
    "gbgs_predictions = gbgs.predict(X_test_pca)\n",
    "print(gbgs_predictions)\n",
    "time= time.clock() - start_time\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA=100, GBC(max_depth=10,n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.clock()\n",
    "pca = PCA(n_components=100,random_state=42)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_std)\n",
    "X_test_pca = pca.fit_transform(X_std_test)\n",
    "\n",
    "gbgs  = GradientBoostingClassifier(max_depth=10,n_estimators=100,random_state=42)\n",
    "\n",
    "\n",
    "gbgs.fit(X_train_pca,Y)\n",
    "\n",
    "gbgs_predictions = gbgs.predict(X_test_pca)\n",
    "print(gbgs_predictions)\n",
    "time= time.clock() - start_time\n",
    "print(\" Time in mins: \",time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explained Variance\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=4)\n",
    "X_lda = lda.fit_transform(X,Y)\n",
    "lda.n_components=4\n",
    "explained_variance = lda.explained_variance_ratio_\n",
    "cum_var_explained=np.cumsum(explained_variance)\n",
    "print(cum_var_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cum_var_explained)\n",
    "plt.axis('tight')  \n",
    "plt.grid() \n",
    "plt.xlabel('n_components') \n",
    "plt.ylabel('Cumulative_Variance_explained')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LDA +SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing LDA with SVM for 60000 samples for 4 features \n",
    "\n",
    "start_time = time.clock()\n",
    "lda = LinearDiscriminantAnalysis(n_components=4)\n",
    "\n",
    "lda.fit(X_std,Y)\n",
    "X_train_lda = lda.transform(X_std)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "grid = SVC(C=60,kernel='rbf')\n",
    "\n",
    "grid.fit(X_train_lda,Y)\n",
    "\n",
    "scores=cross_val_score(grid, X_train_lda,Y, cv=10)\n",
    "print(scores)\n",
    "print(grid_predictions)\n",
    "time= time.clock() - start_time\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LDA+ Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "lda = LinearDiscriminantAnalysis(n_components=4)\n",
    "\n",
    "lda.fit(X_std,Y)\n",
    "X_train_lda = lda.transform(X_std)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "rf = RandomForestClassifier(n_estimators=100,max_depth=10,random_state=42)\n",
    "scores=cross_val_score(rf, X_train_lda,Y, cv=10)\n",
    "\n",
    "print(\"Mean accuracy score when n_estimators = {0} is {1}\".format(100,scores.mean()))\n",
    "rf.fit(X_train_lda,Y )\n",
    "\n",
    "rf_predictions = rf.predict(X_test_lda)\n",
    "print(rf_predictions)\n",
    "time= time.clock() - start_time\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC for PCA+SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting Training Data into Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. For SVC(C=60) and PCA=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y= label_binarize(Y, classes=[0,1,2,3,4])\n",
    "n_classes = 5\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_train_test, Y_train, Y_train_test =\\\n",
    "    train_test_split(X_std, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "pca = PCA(n_components=100,random_state=42)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "#classifier\n",
    "clf = OneVsRestClassifier(SVC(C=60,kernel='rbf',random_state=42))\n",
    "y_score = clf.fit(X_train, Y_train).decision_function(X_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "import matplotlib.pyplot as plt\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes=5\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_train_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "colors = ['aqua', 'darkorange', 'cornflowerblue','green','purple']\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "# Plot of a ROC curve for a specific class\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. For SVC(C=20, Kernel=rbf) and PCA=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50,random_state=42)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "Y= label_binarize(Y, classes=[0,1,2,3,4])\n",
    "n_classes = 5\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_train_test, Y_train, Y_train_test =\\\n",
    "    train_test_split(X_std, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#classifier\n",
    "clf = OneVsRestClassifier(SVC(C=20,kernel='rbf',random_state=42))\n",
    "y_score = clf.fit(X_train, Y_train).decision_function(X_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "import matplotlib.pyplot as plt\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes=5\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_train_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "colors = ['aqua', 'darkorange', 'cornflowerblue','green','purple']\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "# Plot of a ROC curve for a specific class\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other metrics on Training Data for PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with SVM and Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, Y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  a. F1 Score and Log Loss for PCA =100 with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100,random_state=42)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "clf = SVC(C = 60, kernel = 'rbf', random_state=42)\n",
    "scores=cross_val_score(clf, X_train_pca,y_train, cv=10)\n",
    "print(\"Mean accuracy score when C = {0} is {1}\".format(60,scores.mean()))\n",
    "clf.fit(X_train_pca, y_train)\n",
    "predictions = clf.predict(X_test_pca)\n",
    "y_score = clf.fit(X_train_pca, y_train).decision_function(X_test_pca)\n",
    "\n",
    "f1_s = f1_score(y_test, predictions,average=None)\n",
    "print(\"F1 score when C = {0} is {1}\".format(60,f1_s))\n",
    "\n",
    "cf = SVC(C=60, kernel='rbf',probability=True,random_state=42)\n",
    "cf.fit(X_train_pca, y_train)\n",
    "svmpred = cf.predict_proba(X_test_pca)\n",
    "lg_loss = log_loss(y_test, svmpred)\n",
    "print(\"Logarithm loss is {0}\".format(lg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  b. F1 Score and Log Loss for PCA=50+SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50,random_state=42)\n",
    "\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "clf = SVC(C = 20, kernel = 'rbf', random_state=42)\n",
    "scores=cross_val_score(clf, X_train_pca,y_train, cv=10)\n",
    "print(\"Mean accuracy score when C = {0} is {1}\".format(20,scores.mean()))\n",
    "\n",
    "clf.fit(X_train_pca, y_train)\n",
    "predictions = clf.predict(X_test_pca)\n",
    "\n",
    "y_score = clf.fit(X_train_pca, y_train).decision_function(X_test_pca)\n",
    "\n",
    "f1_s = f1_score(y_test, predictions,average=None)\n",
    "print(\"F1 score when C = {0} is {1}\".format(20,f1_s))\n",
    "\n",
    "cf = SVC(C=20, kernel='rbf',probability=True,random_state=42)\n",
    "cf.fit(X_train_pca, y_train)\n",
    "svmpred = cf.predict_proba(X_test_pca)\n",
    "lg_loss = log_loss(y_test, svmpred)\n",
    "print(\"Logarithm loss is {0}\".format(lg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score and Log Loss for PCA+Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestClassifier(n_estimators=100,max_depth=10,random_state=42)\n",
    "scores=cross_val_score(rf, X_train_pca,y_train, cv=10)\n",
    "print(\"Mean accuracy score when n_estimators = {0} is {1}\".format(100,scores.mean()))\n",
    "rf.fit(X_train_pca,y_train )\n",
    "\n",
    "rf_predictions = rf.predict(X_test_pca)\n",
    "f1_s = f1_score(y_test, rf_predictions,average=None)\n",
    "print(\"F1 score when n_estimators = {0} is {1}\".format(100,f1_s))\n",
    "\n",
    "rfpred = rf.predict_proba(X_test_pca)\n",
    "lg_loss = log_loss(y_test, rfpred)\n",
    "print(\"Logarithm loss is {0}\".format(lg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA with SVM for other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=4)\n",
    "\n",
    "lda.fit(X_std,y_train)\n",
    "X_train_lda = lda.transform(X_train)\n",
    "X_test_lda = lda.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 score and Log loss for LDA+SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(C = 60, kernel = 'rbf', random_state=42)\n",
    "scores=cross_val_score(clf, X_train_lda,y_train, cv=10)\n",
    "print(\"Mean accuracy score when C = {0} is {1}\".format(60,scores.mean()))\n",
    "clf.fit(X_train_lda, y_train)\n",
    "predictions = clf.predict(X_test_lda)\n",
    "y_score = clf.fit(X_train_lda, y_train).decision_function(X_test_lda)\n",
    "f1_s = f1_score(y_test, predictions,average=None)\n",
    "print(\"F1 score when C = {0} is {1}\".format(60,f1_s))\n",
    "\n",
    "cf = SVC(C=60, kernel='rbf',probability=True,random_state=42)\n",
    "cf.fit(X_train_lda, y_train)\n",
    "svmpred = cf.predict_proba(X_test_lda)\n",
    "lg_loss = log_loss(y_test, svmpred)\n",
    "print(\"Logarithm loss is {0}\".format(lg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification : Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of your Design Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import io\n",
    "import matplotlib.pyplot as plt # Import matplotlib for data visualisation\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction DataSet from Kaggle for Part-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import io\n",
    "uploaded = files.upload()\n",
    "!pip install -q kaggle\n",
    "\n",
    "# Let's make sure the kaggle.json file is present.\n",
    "!ls -lha kaggle.json\n",
    "\n",
    "# Next, install the Kaggle API client.\n",
    "!pip install -q kaggle\n",
    "\n",
    "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
    "# so move it there.\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# This permissions change avoids a warning on Kaggle tool startup.\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "!kaggle competitions download -c ece657a-w20-asg3-part2\n",
    "\n",
    "!unzip train.csv.zip\n",
    "!unzip testX.csv.zip\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('testX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training and testing dataset \n",
    "X_train= train_data.iloc[:, 2:786].values\n",
    "y_train= train_data.iloc[:,1].values\n",
    "print(X_train.shape)\n",
    "y_train.shape\n",
    "\n",
    "X_test= test_data.iloc[:,1:].values\n",
    "#Y_test =test_data.iloc[:,1].values\n",
    "X_test.shape\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_test = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * unpack the tuple\n",
    "X_train = X_train.reshape(X_train.shape[0], *(28, 28, 1))\n",
    "X_test = X_test.reshape(X_test.shape[0], *(28, 28, 1))\n",
    "X_validate = X_validate.reshape(X_validate.shape[0], *(28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with 1 convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "cnn_model = Sequential()\n",
    "\n",
    "cnn_model.add(Conv2D(32, (3, 3), input_shape = (28,28,1), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "cnn_model.add(Dropout(0.25))\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(units = 512, activation = 'relu'))\n",
    "cnn_model.add(Dropout(0.4))\n",
    "cnn_model.add(Dense(units = 5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss ='sparse_categorical_crossentropy', optimizer=Adam(),metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "history = cnn_model.fit(X_train, y_train, batch_size = 128, epochs = epochs,\n",
    "                        verbose = 1, validation_data = (X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = cnn_model.predict_classes(X_test)\n",
    "print(predicted_classes)\n",
    "time= time.clock() - start_time\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for Training Epoch vs  Accuracy and Training Epoch vs Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with 2 Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "cnn_model = Sequential()\n",
    "\n",
    "\n",
    "cnn_model.add(Conv2D(32, (3, 3), input_shape = (28,28,1), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "cnn_model.add(Conv2D(64, (3, 3), input_shape = (28,28,1), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "cnn_model.add(Dropout(0.25))\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(units = 512, activation = 'relu'))\n",
    "cnn_model.add(Dropout(0.4))\n",
    "cnn_model.add(Dense(units = 5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss ='sparse_categorical_crossentropy', optimizer=Adam(),metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "history = cnn_model.fit(X_train, y_train, batch_size = 128, epochs = epochs,\n",
    "                        verbose = 1, validation_data = (X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions for the test data\n",
    "predicted_classes = cnn_model.predict_classes(X_test)\n",
    "print(predicted_classes)\n",
    "time= time.clock() - start_time\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with 3 convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "cnn_model = Sequential()\n",
    "\n",
    "# Try 32 fliters first then 64\n",
    "cnn_model.add(Conv2D(32, (3, 3), input_shape = (28,28,1), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "cnn_model.add(Conv2D(64, (3, 3), input_shape = (28,28,1), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "cnn_model.add(Conv2D(128, (3, 3), input_shape = (28,28,1), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "cnn_model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(units = 512, activation = 'relu'))\n",
    "cnn_model.add(Dropout(0.4))\n",
    "cnn_model.add(Dense(units = 5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss ='sparse_categorical_crossentropy', optimizer=Adam(),metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "history = cnn_model.fit(X_train, y_train, batch_size = 128, epochs = epochs,\n",
    "                        verbose = 1, validation_data = (X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions for the test data\n",
    "predicted_classes = cnn_model.predict_classes(X_test)\n",
    "print(predicted_classes)\n",
    "time= time.clock() - start_time\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet- for Epoch=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import MaxPooling2D, AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "#from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('testX.csv')\n",
    "\n",
    "# Prepare the training and testing dataset \n",
    "X_train= train_data.iloc[:, 2:786].values\n",
    "Y_train= train_data.iloc[:,1].values\n",
    "X_train.shape\n",
    "\n",
    "X_test= test_data.iloc[:,1:].values\n",
    "#Y_test =test_data.iloc[:,1].values\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining constants\n",
    "epochs = 25\n",
    "batch_size = 100\n",
    "data_augmentation = False\n",
    "img_size = 28\n",
    "\n",
    "num_classes = 5\n",
    "num_filters = 64\n",
    "num_blocks = 4\n",
    "num_sub_blocks = 2\n",
    "use_max_pool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.reshape(X_train.shape[0],img_size,img_size,1)\n",
    "x_test = X_test.reshape(X_test.shape[0],img_size,img_size,1)\n",
    "input_size = (img_size, img_size,1)\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "\n",
    "#Converting labels to one-hot vectors\n",
    "y_train = keras.utils.to_categorical(Y_train, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "#Creating model based on ResNet published archietecture\n",
    "inputs = Input(shape=input_size)\n",
    "x = Conv2D(num_filters, padding='same', \n",
    "           kernel_initializer='he_normal', \n",
    "           kernel_size=7, strides=2,\n",
    "           kernel_regularizer=l2(1e-4))(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#Check by applying max pooling later (setting it false as size of image is small i.e. 28x28)\n",
    "if use_max_pool:\n",
    "    x = MaxPooling2D(pool_size=3,padding='same', strides=2)(x)\n",
    "    num_blocks =3\n",
    "#Creating Conv base stack \n",
    "\n",
    "# Instantiate convolutional base (stack of blocks).\n",
    "for i in range(num_blocks):\n",
    "    for j in range(num_sub_blocks):\n",
    "        strides = 1\n",
    "        is_first_layer_but_not_first_block = j == 0 and i > 0\n",
    "        if is_first_layer_but_not_first_block:\n",
    "            strides = 2\n",
    "        #Creating residual mapping using y\n",
    "        y = Conv2D(num_filters,\n",
    "                   kernel_size=3,\n",
    "                   padding='same',\n",
    "                   strides=strides,\n",
    "                   kernel_initializer='he_normal',\n",
    "                   kernel_regularizer=l2(1e-4))(x)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Activation('relu')(y)\n",
    "        y = Conv2D(num_filters,\n",
    "                   kernel_size=3,\n",
    "                   padding='same',\n",
    "                   kernel_initializer='he_normal',\n",
    "                   kernel_regularizer=l2(1e-4))(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        if is_first_layer_but_not_first_block:\n",
    "            x = Conv2D(num_filters,\n",
    "                       kernel_size=1,\n",
    "                       padding='same',\n",
    "                       strides=2,\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l2(1e-4))(x)\n",
    "        #Adding back residual mapping\n",
    "        x = keras.layers.add([x, y])\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2 * num_filters\n",
    "\n",
    "# Add classifier on top.\n",
    "x = AveragePooling2D()(x)\n",
    "y = Flatten()(x)\n",
    "outputs = Dense(num_classes,\n",
    "                activation='softmax',\n",
    "                kernel_initializer='he_normal')(y)\n",
    "\n",
    "# Instantiate and compile model.\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(x_test)\n",
    "pred=np.argmax(pred,axis=1)\n",
    "time= time.clock() - start_time\n",
    "print(time)\n",
    "df=pd.DataFrame(pred)\n",
    "df.index+=0\n",
    "df.index.name='Id'\n",
    "df.columns=['Label']\n",
    "df.to_csv('resnet1.csv',header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for Training Epoch vs  Accuracy and Training Epoch vs Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    #val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    #val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    #plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training accuracy')\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    #plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet- Epoch=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('testX.csv')\n",
    "\n",
    "# Prepare the training and testing dataset \n",
    "X_train= train_data.iloc[:, 2:786].values\n",
    "Y_train= train_data.iloc[:,1].values\n",
    "X_train.shape\n",
    "\n",
    "X_test= test_data.iloc[:,1:].values\n",
    "#Y_test =test_data.iloc[:,1].values\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining constants\n",
    "epochs = 25\n",
    "batch_size = 100\n",
    "data_augmentation = False\n",
    "img_size = 28\n",
    "\n",
    "num_classes = 5\n",
    "num_filters = 64\n",
    "num_blocks = 4\n",
    "num_sub_blocks = 2\n",
    "use_max_pool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.reshape(X_train.shape[0],img_size,img_size,1)\n",
    "x_test = X_test.reshape(X_test.shape[0],img_size,img_size,1)\n",
    "input_size = (img_size, img_size,1)\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "\n",
    "#Converting labels to one-hot vectors\n",
    "y_train = keras.utils.to_categorical(Y_train, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "#Creating model based on ResNet published archietecture\n",
    "inputs = Input(shape=input_size)\n",
    "x = Conv2D(num_filters, padding='same', \n",
    "           kernel_initializer='he_normal', \n",
    "           kernel_size=7, strides=2,\n",
    "           kernel_regularizer=l2(1e-4))(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#Check by applying max pooling later (setting it false as size of image is small i.e. 28x28)\n",
    "if use_max_pool:\n",
    "    x = MaxPooling2D(pool_size=3,padding='same', strides=2)(x)\n",
    "    num_blocks =3\n",
    "#Creating Conv base stack \n",
    "\n",
    "# Instantiate convolutional base (stack of blocks).\n",
    "for i in range(num_blocks):\n",
    "    for j in range(num_sub_blocks):\n",
    "        strides = 1\n",
    "        is_first_layer_but_not_first_block = j == 0 and i > 0\n",
    "        if is_first_layer_but_not_first_block:\n",
    "            strides = 2\n",
    "        #Creating residual mapping using y\n",
    "        y = Conv2D(num_filters,\n",
    "                   kernel_size=3,\n",
    "                   padding='same',\n",
    "                   strides=strides,\n",
    "                   kernel_initializer='he_normal',\n",
    "                   kernel_regularizer=l2(1e-4))(x)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Activation('relu')(y)\n",
    "        y = Conv2D(num_filters,\n",
    "                   kernel_size=3,\n",
    "                   padding='same',\n",
    "                   kernel_initializer='he_normal',\n",
    "                   kernel_regularizer=l2(1e-4))(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        if is_first_layer_but_not_first_block:\n",
    "            x = Conv2D(num_filters,\n",
    "                       kernel_size=1,\n",
    "                       padding='same',\n",
    "                       strides=2,\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l2(1e-4))(x)\n",
    "        #Adding back residual mapping\n",
    "        x = keras.layers.add([x, y])\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2 * num_filters\n",
    "\n",
    "# Add classifier on top.\n",
    "x = AveragePooling2D()(x)\n",
    "y = Flatten()(x)\n",
    "outputs = Dense(num_classes,\n",
    "                activation='softmax',\n",
    "                kernel_initializer='he_normal')(y)\n",
    "\n",
    "# Instantiate and compile model.\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(x_test)\n",
    "pred=np.argmax(pred,axis=1)\n",
    "time= time.clock() - start_time\n",
    "print(time)\n",
    "df=pd.DataFrame(pred)\n",
    "df.index+=0\n",
    "df.index.name='Id'\n",
    "df.columns=['Label']\n",
    "df.to_csv('resnet1.csv',header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Other Metrics with 1 convolutional layer - Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "\n",
    "# Prepare the training and testing dataset \n",
    "X= train_data.iloc[:, 2:786].values\n",
    "y= train_data.iloc[:,1].values\n",
    "print(X.shape)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], *(28, 28, 1))\n",
    "X_test = X_test.reshape(X_test.shape[0], *(28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "\n",
    "# Try 32 fliters first then 64\n",
    "cnn_model.add(Conv2D(32, (3, 3), input_shape = (28,28,1), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "cnn_model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(units = 512, activation = 'relu'))\n",
    "cnn_model.add(Dropout(0.4))\n",
    "cnn_model.add(Dense(units = 5, activation = 'softmax'))\n",
    "\n",
    "cnn_model.compile(loss ='sparse_categorical_crossentropy', optimizer=Adam(),metrics =['accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "history = cnn_model.fit(X_train, y_train, batch_size = 128, epochs = epochs,\n",
    "                        verbose = 1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cnn_model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Loss: {:.4f}'.format(score[0]))\n",
    "print('Accuracy: {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['0','1','2','3','4']\n",
    "print(classification_report(y_test, predicted_classes, target_names = classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet Other Metrics for Epoch=25 - Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import MaxPooling2D, AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "#from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('testX.csv')\n",
    "\n",
    "# Prepare the training and testing dataset \n",
    "X= train_data.iloc[:, 2:786].values\n",
    "Y= train_data.iloc[:,1].values\n",
    "print(X.shape)\n",
    "Y.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "#Defining constants\n",
    "epochs = 25\n",
    "batch_size = 100\n",
    "data_augmentation = False\n",
    "img_size = 28\n",
    "\n",
    "num_classes = 5\n",
    "num_filters = 64\n",
    "num_blocks = 4\n",
    "num_sub_blocks = 2\n",
    "use_max_pool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.reshape(X_train.shape[0],img_size,img_size,1)\n",
    "x_test = X_test.reshape(X_test.shape[0],img_size,img_size,1)\n",
    "input_size = (img_size, img_size,1)\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('Y_train shape:', y_train.shape)\n",
    "\n",
    "#Converting labels to one-hot vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating model based on ResNet published archietecture\n",
    "inputs = Input(shape=input_size)\n",
    "x = Conv2D(num_filters, padding='same', \n",
    "           kernel_initializer='he_normal', \n",
    "           kernel_size=7, strides=2,\n",
    "           kernel_regularizer=l2(1e-4))(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#Check by applying max pooling later (setting it false as size of image is small i.e. 28x28)\n",
    "if use_max_pool:\n",
    "    x = MaxPooling2D(pool_size=3,padding='same', strides=2)(x)\n",
    "    num_blocks =3\n",
    "#Creating Conv base stack \n",
    "\n",
    "# Instantiate convolutional base (stack of blocks).\n",
    "for i in range(num_blocks):\n",
    "    for j in range(num_sub_blocks):\n",
    "        strides = 1\n",
    "        is_first_layer_but_not_first_block = j == 0 and i > 0\n",
    "        if is_first_layer_but_not_first_block:\n",
    "            strides = 2\n",
    "        #Creating residual mapping using y\n",
    "        y = Conv2D(num_filters,\n",
    "                   kernel_size=3,\n",
    "                   padding='same',\n",
    "                   strides=strides,\n",
    "                   kernel_initializer='he_normal',\n",
    "                   kernel_regularizer=l2(1e-4))(x)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Activation('relu')(y)\n",
    "        y = Conv2D(num_filters,\n",
    "                   kernel_size=3,\n",
    "                   padding='same',\n",
    "                   kernel_initializer='he_normal',\n",
    "                   kernel_regularizer=l2(1e-4))(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        if is_first_layer_but_not_first_block:\n",
    "            x = Conv2D(num_filters,\n",
    "                       kernel_size=1,\n",
    "                       padding='same',\n",
    "                       strides=2,\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l2(1e-4))(x)\n",
    "        #Adding back residual mapping\n",
    "        x = keras.layers.add([x, y])\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2 * num_filters\n",
    "\n",
    "# Add classifier on top.\n",
    "x = AveragePooling2D()(x)\n",
    "y = Flatten()(x)\n",
    "outputs = Dense(num_classes,\n",
    "                activation='softmax',\n",
    "                kernel_initializer='he_normal')(y)\n",
    "\n",
    "# Instantiate and compile model.\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(x_test)\n",
    "pred=np.argmax(pred,axis=1)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['0','1','2','3','4']\n",
    "test_label=np.argmax(y_test, axis=1)\n",
    "test_label[1]\n",
    "print(classification_report(test_label, pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "<ol>\n",
    "    Question 1:\n",
    "<li>https://statinfer.com/204-6-8-svm-advantages-disadvantages-applications/</li>\n",
    "<li>https://www.knowledgehut.com/blog/data-science/linear-discriminant-analysis-for-machine-learning</li>\n",
    "<li>https://stackoverflow.com/questions/45332410/sklearn-roc-for-multiclass-classification</li>\n",
    "<li> https://www.i2tutorials.com/top-machine-learning-interview-questions-and-answers/what-are-the-pros-and-cons-of-the-pca/</li>\n",
    "<li>https://medium.com/machine-learning-researcher/dimensionality-reduction-pca-and-lda-6be91734f567</li>\n",
    "<li>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1\\_score.html</li>\n",
    "<li> https://medium.com/analytics-vidhya/accuracy-vs-f1-score-6258237beca2</li>\n",
    "<li>https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "</li>\n",
    "<li> https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log\\_loss.html</li>\n",
    "<li> https://stackoverflow.com/questions/33616102/sklearn-log-loss-different-number-of-classes/42953278</li>\n",
    "<li> https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989</li>\n",
    "<li>https://chrisalbon.com/machine\\_learning/support\\_vector\\_machines/svc\\_parameters\\_using\\_rbf\\_kernel/</li>\n",
    "<li>https://www.kdnuggets.com/2016/06/select-support-vector-machine-kernels.html</li>\n",
    "<li>https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b</li>\n",
    "<li> https://data-flair.training/blogs/gradient-boosting-algorithm/</li>\n",
    "<li>https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting</li>\n",
    "<li>https://stats.stackexchange.com/questions/258938/pca-before-random-forest-regression-provide-better-predictive-scores-for-my-data/258942</li>\n",
    "<li> https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74</li>\n",
    "<li> https://towardsdatascience.com/machine-learning-step-by-step-6fbde95c455a</li>\n",
    "<li>https://www.kaggle.com/residentmario/dimensionality-reduction-and-pca-for-fashion-mnist</li>\n",
    "<li>https://www.kaggle.com/lonewolf95/classification-tutorial-with-pca-and-gridsearchcv</li>\n",
    "<li>https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76</li>\n",
    "<li>https://github.com/dask/dask-ml/issues/159</li>\n",
    "<li> https://towardsdatascience.com/hyperparameter-optimization-with-keras-b82e6364ca53</li>\n",
    "<li>https://machinelearningmastery.com/make-predictions-scikit-learn/</li>\n",
    "<li>https://stackabuse.com/implementing-pca-in-python-with-scikit-learn/</li>\n",
    "<li>https://medium.com/@sarayupgouda/pca-principal-component-analysis-in-python-f9836c25acb9 https://stackabuse.com/implementing-pca-in-python-with-scikit-learn/</li>\n",
    "<li> https://go2analytics.wordpress.com/2016/07/26/implement-classification-in-python-and-roc-plotting-svc-example/</li>\n",
    "    \n",
    "    \n",
    "</ol>    \n",
    "    Question 2:\n",
    "<li>https://www.machinecurve.com/index.php/2019/09/17/how-to-create-a-cnn-classifier-with-keras/</li>\n",
    "<li>https://pravarmahajan.github.io/fashion/</li>\n",
    "<li>https://github.com/cmasch/zalando-fashion-mnist</li>\n",
    "<li>https://github.com/zalandoresearch/fashion-mnist</li>\n",
    "<li>http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/</li>\n",
    "<li>https://github.com/nuclearczy/SVM\\_and\\_CNN\\_on\\_Fashion\\_MNIST\\_Dataset</li>\n",
    "<li>https://github.com/shoji9x9/Fashion-MNIST-By-ResNet/blob/master/Fashion-MNIST-by-ResNet-50.ipynb</li>\n",
    "<li>https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33</li>\n",
    "<li>https://www.kaggle.com/girishgupta/fashion-mnist-using-resnet</li>\n",
    "<li>https://towardsdatascience.com/introduction-to-resnets-c0a830a288a4</li>\n",
    "<li>https://stackoverflow.com/questions/54589669/confusion-matrix-error-classification-metrics-cant-handle-a-mix-of-multilabel</li>\n",
    "<li>https://www.kaggle.com/akumaldo/resnet-from-scratch-keras</li>\n",
    "<li>https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234</li>\n",
    "<li>https://towardsdatascience.com/the-4-convolutional-neural-network-models-that-can-classify-your-fashion-images-9fe7f3e5399d</li>\n",
    "<li>https://keras.io/</li>\n",
    "<li>https://bmcproc.biomedcentral.com/articles/10.1186/1753-6561-5-S3-S11</li>\n",
    "<li>https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33</li>\n",
    "<li>https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/</li>\n",
    "<li>https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9</li>\n",
    "<li>https://www.kaggle.com/faressayah/fashion-classification-mnist-cnn-tutorial</li>\n",
    "\n",
    "\n",
    "</ol>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
